import requests
from bs4 import BeautifulSoup
from datetime import datetime
import csv
import lxml

#URL used for reference
url = "Enter your URL here"
response = requests.get(url, stream = True)

# parse html
page = str(BeautifulSoup(response.content, 'lxml'))

dataList = list()

def getURL(page):
    start_link = page.find("a href")
    if start_link == -1:
        return None, 0
    start_quote = page.find('"', start_link)
    end_quote = page.find('"', start_quote + 1)
    url = page[start_quote + 1: end_quote]
    return url, end_quote

while True:
    url, n = getURL(page)
    page = page[n:]
    if url:
        dataList.append(url)
        #print(root+url) #Uncomment this line if you want to see the output in the terminal.
    else:
        break

# Writing the URLs to a text file

with open('filename.txt', "w") as fhandle:
  for line in dataList:
    fhandle.write(f'{line}\n')
